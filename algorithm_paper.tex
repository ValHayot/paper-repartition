\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithmic, algorithm}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ Dask IO for sequentially splitting, merging and resplitting multidimensional arrays }

\author{\IEEEauthorblockN{1\textsuperscript{st} Timothée Guédon}
\IEEEauthorblockA{\textit{Department of Computer Science and Software Engineering} \\
\textit{Concordia University}\\
Montreal, Quebec, Canada \\
t\_guedon@encs.concordia.ca}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Tristan Glatard}
\IEEEauthorblockA{\textit{Department of Computer Science and Software Engineering} \\
\textit{Concordia University}\\
Montreal, Quebec, Canada \\
tristan.glatard@concordia.ca}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Valérie Hayot-Sasson}
\IEEEauthorblockA{\textit{Department of Computer Science and Software Engineering} \\
\textit{Concordia University}\\
Montreal, Quebec, Canada \\
valerie.hayot-sasson@concordia.ca}
}

\maketitle

\begin{abstract}
% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
% or Math in Paper Title or Abstract.
In this paper, we define a new algorithm to resplit a multidimensional array stored into a set of files, and we show that it can also be used to split and merge data with the same behavior than the ``multiple" strategy. We also give an implementation of this algorithm into a Python library called ``Dask IO", available as an optimization package for the Python big data library Dask.
\end{abstract}

\begin{IEEEkeywords}
multidimensional, array, split, merge, resplit, IO, processing, Dask, Python
\end{IEEEkeywords}

%----------------------------------------
\section*{Introduction}
%----------------------------------------

\subsection{Context}

With the improvement of aquisition methods in several scientific domains like health sciences, geology and astrophysics, new big data challenges have emerged. It includes processing a big amount of data, as well as processing very big files like ultra high resolution images. An example of a ultra high resolution image in the neuroscience field is Big Brain, a model of the brain providing microscopic data (20 micrometers) for modeling and simulation. Most image processing pipelines process only specific ROI (Region Of Interest) or process data by block. At the end of such pipelines, merging back those blocks into one reconstructed image may be required as well. The problem is that naive algorithms perform poorly due to the amount of seeks occuring on disk\cite{seqalgorithms}. Previous work in \cite{seqalgorithms} introduced two types of sequential algorithms to split and merge multidimensional arrays: the ``clustered" and ``multiple" strategies. To the best of our knowedge however, no algorithm has been proposed for the resplit task. Moreover, the split and merge tasks are special cases of the resplit task, which gives an opportunity to solve the three problems at once. It is easy to see that the ``clustered" and ``multiple" strategies are non optimal for the resplit task, that is why we searched for another strategy.

\subsection{Problem definition}

Consider a multidimensional array of shape $R = (R_i, R_j, R_k)$, stored in some input files with a given shape $I = (I_i, I_j, I_k)$, all input files having the same shape.
Our goal is to optimize the process of sequentially resplitting the input files into output files with a different shape $O = (O_i, O_j, O_k)$, all output files having the same shape, too. \\

The resplit process has two particular cases:
\begin{itemize}
  \item It becomes a split process if there is one input file and several output files,
  \item It becomes a merge process if there are several input files and one output file.
\end{itemize}

For this I/O process to be fast, one needs to minimize the number of seeks that occur on disk while reading and writing.
We consider that a seek occurs either when opening a file or seeking into a file. \\

Let us consider a basic sequential resplit algorithm: One can repeatedly read the maximum amount of data possible from the input files into a buffer stored in main memory, and then write this buffer down into the output files requiring this data, until all output files have been completely written.
This resplit algorithm is described in Algorithm \ref{algo:generalresplit}. \\

The algorithm described in Algorithm \ref{algo:generalresplit} takes a list of input and output files $inFiles$ and $outFiles$ as parameters, as well as the amount of memory $m$ available in RAM for the buffer and the list of the buffers' coordinates. We call $m'$ the amount of memory available in the buffer at a given time during the execution of the algorithm ($m'=m$ at initialization). The list of buffers' coordinates contains the coordinates of each buffer to be loaded in the referential of the reconstructed image. \\

The algorithm successively loads as much data as it can from the input files into the buffer and write it down to the output files that are supposed to contain this data.
Although we could use a naive shape for the buffer, we can use the input and output files shapes to elaborate more efficient strategies as we will see in the next sections.
The algorithm ends when all buffers have been read.
Therefore, the buffers must cover the whole reconstructed image such that when the algorithm end all the output files have been completely written.
For the latter statement to be true, we also need to ensure that all data read from input files are either stored in RAM or directly written, such that all the output files are completely written at the end of the algorithm. \\

Given Algorithm \ref{algo:generalresplit}, the optimization problem that we want to solve can be stated as follows:
Given the amount of main memory available $m$, as well as the shapes of the input and output files $I$ and $O$, how to select the best buffer shape $B$ which will minimize the number of seeks that take place during reading and writing? \\

We add two restrictions on the buffers: We shall use only non-overlaping buffers, all buffers having the same size, and each buffer has to be written only once.

\begin{algorithm}[H]
  \caption{Basic resplit algorithm}
  \label{algo:generalresplit}
  \begin{algorithmic}
    \STATE \textbf{Inputs:} {inFiles, outFiles, m, buffersList}
    \FOR{$\textrm{buffer in buffersList}$}
      \STATE $\textrm{read(inFiles, buffer)}$
      \STATE $\textrm{write(outFiles, buffer)}$
    \ENDFOR

  \end{algorithmic}
\end{algorithm}

\subsection{Consistency with previous works}
For the sake of consistency with previous works \cite{seqalgorithms}, we call the original array of shape $R$ stored in the input files the ``reconstructed image" (see Figure \ref{fig:reconstructed_img_divided}).
Of course, the input files' positions in the reconstructed image have to be stored in some way.
Also with a view to be consistent with previous works, we assume the files to be written in column-order.
In column ordering (also called ``F" ordering) the fastest moving dimension is the last dimension of the array and the slowest moving dimension is the first dimension.
For example a 3D array with dimensions i, j and k would be written on disk by writing the complete columns in the k dimension first (see Figure \ref{fig:column_order}).

\begin{figure}[h!]
\centering
\includegraphics[scale=0.2]{./figures/reconstructed_img_divided.png}
\caption{Illustration of the reconstructed image divided into input files at the initialization of the resplit algorithm.
}
\label{fig:reconstructed_img_divided}
\end{figure}

\begin{figure*}[h!]
\centering
\includegraphics[scale=0.2]{./figures/column_order.png}
\caption{Illustration of the column-order storage of voxels in a file.
}
\label{fig:column_order}
\end{figure*}

\subsection{Naive algorithm}

As a base case for our problem, let us define a naive algorithm which loads one input file at a time and write it down into the different output files that requires the data.
The buffers have the same shape as the input files ($B=I$) and the order is the same order than the storage order i.e. the column order in this study ($[k, j, i]$ order).
% TODO : seeks analysis

\subsection{A particular case}

If the input shape is a multiple of the output shape such that one input file covers several output files entirely without falls, then the problem is easily solved: one must read as much input files as one, i.e. the buffer shape $B$ is a multiple of the input shape $I$.
The algorithm will produce one seek per input file and one seek to write each output file, which is the minimum number of seeks possible for a resplit.\\

If there is a mismatch between the shapes in any dimension however, one needs a strategy to manage with this overlap while minimizing the number of seeks.
We will introduce a strategy to keep falls temporarily into memory in the next section, this strategy's efficiency is completely dependent on the amount of main memory available. \\

Also, the resplit process requires multiple buffers to be read.
If there is no overlap between input and output files, then the order in which we load buffers is not important.
In case of an overlap however, the order may have an important impact on the number of seeks produced.

%----------------------------------------
\section*{The ``keep" algorithm}
%----------------------------------------
The algorithm presented in this paper is called the ``keep" algorithm, as it relies on a so-called ``keep strategy" that is presented below.
The ``keep" algorithm is an algorithm to find the best buffer shape for a resplit.

\subsection{The ``keep" strategy}
At initialization, we assign a shape $B = (0, 0,..., 0)$ for the buffer.
We will then stretch the buffer in each dimension until all the available memory $m$ has been used while keeping the number of seeks as small as possible. \\

Let us consider the first dimension $f$ that we increase: Ideally, one wants the buffer to cover both the input file and the output file, $I_f$ and $O_f$, such that we read and write in one seek each.
If there is a mismatch between $O_f$ and $I_f$ (meaning one is not a multiple of the other) we want to read a multiple of $O_f$ or $I_f$ such that we either read partially or write partially but not both at the same time.
We must therefore choose between reading a multiple of $O_f$ or $I_f$.
Reading a multiple of $I_f$ in the case of a shape mismatch is equivalent to $B_f = nI_f = mO_f+xO_f$ with $0<x<1$, $n$ and $m$ are integers.
We call ``extra data" the data contained in the overlap areas/volumes between input and output shapes (Figure \ref{fig:overlap}).
In this example the extra data is $xO_f$.
An output file that is involved in an overlap is called an ``incomplete output file". \\

\begin{figure}[h]
\centering
\includegraphics[scale=0.2]{./figures/overlap.png}
\caption{Illustration of the concept of extra data with a 2D case. In this example the black bordered rectangles represent the input files and the gray bordered rectangles represent the output files. After having read the red buffer we can see that the output file covered by the light red area can be written directly. After having written the data from the light red area however, we are left with the dark red, dotted area which represents some extra data we would like to keep in memory until we read the rest of the incomplete ouptut files in order to read the incomplete output files with one seek.
}
\label{fig:overlap}
\end{figure}

\begin{figure*}[h]
\centering
\includegraphics[scale=0.2]{./figures/keep_strategy.png}
\caption{Illustration of the keep strategy with a 2D case. In this example the black bordered rectangles represent the input files and the gray bordered rectangles represent the output files. As shown on the left figure, the keep strategy consists in reading more than one output file (light red area) into the first buffer. Then the first output file is written and the dark red area is kept in memory. On the left figure the second buffer has been loaded. It allows to free part of the overlap in the $k$ direction (the green area) as the second file data is complete in main memory. The third output file has been read completely and can therefore be written. This lets the overlap in the $j$ direction (dark red area) in main memory for the next buffer.
}
\label{fig:keepstrategy}
\end{figure*}

One can try to keep the extra data in memory instead of writing it directly into the output file: this is what we call the ``keep strategy" (Figure \ref{fig:keepstrategy}).
The idea is to read ``more than necessary" from the input files, ideally reading each input file in one seek, and to keep as much extra data in memory as possible.
The goal of this strategy is to read input files in one seek, and write as much output files as possible in one seek as well.
If we were to read a multiple of $O_f$ however, we would read an intput file partially and there is nothing that one can do about it.
It may be that we can only keep some of the overlaps in memory as it could take too much buffers until we can write an incomplete output file in one seek without running out of memory.
Extra data that cannot be kept in memory is written directly in the output file(s).
By doing so we ensure that when the algorithm ends all the data has successfully been written.

\subsection{Input aggregates}
We call ``input aggregate", $\Lambda$, the aggregate of the minimum number of input files that covers one output file entirely in all dimensions (see Figure \ref{fig:input_aggregates}, Figure \ref{fig:firstbuffer}).
The input aggregate is the minimal buffer shape we would like to have to be capable of minimizing the number of seeks as it allows to read the input files in one seek and to use the keep strategy.
The mismatch between the input and output shapes and the limited memory available for the buffer may prevent it to be possible. \\

\begin{figure}[h]
\centering
\includegraphics[scale=0.2]{./figures/first_buffer.png}
\caption{First buffer in the column-order storage.
}
\label{fig:firstbuffer}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.15]{./figures/input_aggregates.png}
\caption{Illustration of an input aggregate in two dimensions. On the left schema, consider the surface containing all rectangles as being the reconstructed image, with the small rectangles (with black borders) being the input files and the big rectangles (with gray borders) being the output files. On the right side an input aggregate is illustrated by the red area: it is the smallest number of input files such that the surface of at least one output file is completely covered. In this example, four input files are required to cover the first output file.
}
\label{fig:input_aggregates}
\end{figure}

% We call ``output aggregate", $\Omega$, the aggregate of the output files that are being written by the $i^{th}$ buffer. It includes the incomplete output files for which the missing data have been loaded by the $i^{th}$ buffer. \\

\subsection{Stretching the buffer in the storage order}
The first step of the algorithm is therefore to make the dimensions of the buffer match the dimensions of the input aggregate as much as possible (we want $B=\Lambda$).
If more memory is available after that, we may want to increase the buffer's dimension even further (it is discussed in subsection ``Stretching beyond the input aggregate shape" below).
The question of which dimension should be increased first remains.
One should increase the buffer's dimensions in the order of the fastest moving dimensions.
For example if one is processing a 3D image stored in files following the column-order, one should increase the buffer in the $k$ dimension first, then the $j$ dimension and finally the $i$ dimension. \\

\begin{figure*}[h]
\centering
\includegraphics[scale=0.20]{./figures/case_1_2.png}
\caption{Left figure: without keep strategy.
Right: with keep strategy.
Keeping extra data in memory reduces the number of seeks caused by writing but increases the number of buffers needed to write an output file.
The crosses represent the number of seeks that happen in both cases.
}
\label{fig:case_1_2}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[scale=0.20]{./figures/case_2_1.png}
\caption{Illustration of an overlap in the $j$ dimension. The black crosses represents one seek each. In the 2D case on the left we can see that writing down the data into the next output file on the right would take only one seek. In the 3D case however (right side), writing the data down into the next output file would take $B_i$ seeks.
}
\label{fig:case_2_1}
\end{figure*}

Let us first consider the 2D case in which the input and output files overlap in one dimension only.
An overlap occuring in the $k$ dimension would incur $B_k$ seeks as opposed to 1 seek only (see Figure \ref{fig:case_1_2}) in the case of an overlap in the $j$ dimension (see Figure \ref{fig:case_2_1}).
Note that the keep strategy is not useful if we cannot store the extra data in one direction completely (see Figure \ref{fig:case_2_1}). \\

Let us define 3 overlaping areas, represented in Figure \ref{fig:areasabc}.
The $a$ area is the overlap in the $k$ axis.
The $b$ area is the upper right overlap which combines the overlaps in both axis.
One can see the $b$ area as the continuity of the $a$ area.
The $c$ area is the overlap in the $j$ axis only.
The $a$ area becomes the $A$ volume in 3D, the $B$ volume corresponds to the $b$ area, etc. \\

\begin{figure}[h]
\centering
\includegraphics[scale=0.20]{./figures/areasabc.png}
\caption{Illustration of the different overlaping areas in 2D.
The blue area is called the $a$ area, the red area is called the $b$ area and we call $c$ area the green one.
}
\label{fig:areasabc}
\end{figure}

In terms of the number of seeks, $$\Omega_j > (\Lambda_j-\Omega_j) > 1 => \textrm{seeksIn(a)} > \textrm{seeksIn(b)} > \textrm{seeksIn(c)}$$ in 2D, and this stays true when increasing the number of dimensions.
We must therefore use the memory available for stretching the buffer to store $a$ first, then $b$, and finally $c$.
The same way that it is not useful to save part of the $c$ area as opposed to storing the whole $c$ area, it is not useful to inrease $B_j$ if $B_k < \Lambda_k$.
Indeed, the number of seeks will be the same than without using the keep strategy.
As the $b$ area is the continuity of the $a$ area in the $j$ direction, if the available buffer memory $m'$ allows to store the $b$ area but not the $b$ and $c$ areas together it seems that increasing $B_j$ from $\Omega_j$ to $\Lambda_j$ while storing only the $b$ area could save a lot of seeks. \\

As a conclusion, we should increase the fastest moving dimension first to allow the reduction of the number of seeks in the $a$ and $b$ areas in the $k$ direction.
Then we should increase $B_j$ as much as possible, while saving only $a$ and then $b$, even if we cannot cover it entirely.
Finally, if we can store the whole $c$ area, we should do it before inreasing the $i$ dimension to prevent the 1 seek to be multiplied by $B_i$. \\

\subsection{Stretching beyond the input aggregate shape}
Using small input and output file shapes, it is probable for the buffer to have been stretched to the input aggregate, which means that a good amount of RAM is still available.
One have no reason to stop increasing the shape of the buffer from that point.
The first idea which comes to mind is to stretch the buffer such that the buffer length becomes a multiple of the output file length in a given dimension $x$: $B_x = nO_x$.
Again, this would create an overlap in the input files however, which would prevent the keep strategy to be applied and create a maximum number of seeks.
Therefore, one can only stretch the buffer by adding one input file length to a given dimension of the buffer shape (see Figure \ref{fig:extendingbuffers}).
A good idea seems to be to extend the buffer shape in the dimension of the biggest overlap area/volume such that we accumulate extra data from the smallest overlap volumes only.
Again, if we can stretch the buffer such that $B_x = R_x$ in the dimension $x$ of the biggest overlap area/volume, we then increase in the dimension of the second biggest overlap size, etc.

\begin{figure*}[h]
\centering
\includegraphics[scale=0.20]{./figures/extendingbuffers.png}
\caption{ Left side: Extending the buffer to the next input length. Right side: Extending the buffer to the next output length. The crosses indicates the number of seeks. As we can see, extending the buffer to the next output shape incurs a lot of seeks in the next input file when loading the buffer. As the input file is incomplete, there is no extra data, therefore the keep strategy cannot be used.
}
\label{fig:extendingbuffers}
\end{figure*}

\subsection{Pseudo-code of the algorithm}
The algorithm is described in Algorithm \ref{algo:keep_algorithm_3D}. \\

Details:
\begin{itemize}
  \item line 15: sort the axes in decreasing order of overlap cuboid size
  \item line 16: for each dimension in this order, try to increase the buffer size
  \item lines 17, 18: keep adding the length of an input aggregate in that dimension while there is memory left.
\end{itemize}

Nomenclature:
\begin{itemize}
  \item $A_i$: Area of the $i^{th}$ component
  \item $m$: Amount of main memory available for the buffer at initialization
  \item $m'$: Amount of main memory available for the buffer during the algorithm execution
  \item $\textrm{iterable}[i]$: Access the $i^{th}$ element of an iterable
  \item $B = (B_i, B_j, B_k)$: Shape of the buffer
  \item $(B_i B_j B_k)$: Size of the buffer in voxels
  \item $R_i, R_j, R_k$: Length of the reconstructed image in the $i^{th}$ dimension
  \item $I_i, I_j, I_k$: Length of an input file in the $i^{th}$ dimension
  \item $O_i, O_j, O_k$: Length of an output file in the $i^{th}$ dimension
  \item $C_i(x), C_j(x), C_k(x)$: Overlap size in voxels in the $i^{th}$ dimension, when loading the $t^{th}$ buffer.
  \item $\Lambda_i, \Lambda_j, \Lambda_k$: Length of an input aggregate in the $i^{th}$ dimension
  % \item $\Omega_i, \Omega_j, \Omega_k$: Length of an output aggregate in the $i^{th}$ dimension
  \item $b_i, b_j, b_k = \frac{R_i}{B_i}$: The number of buffers in the $i^{th}$ dimension in the reconstructed image
  \item $f_1, f_2, f_3$: Names of the overlap areas in 2D
  \item $F_1, F_2, F_3, F_4$: Names of the overlap volumes in 3D
  \item $ N_s $: Number of buffers in a slice
  \item $ n_s $: Number of buffers read so far from the current slice.
\end{itemize}

\begin{algorithm}[h]
  \caption{Pseudocode of the ``keep" algorithm in 3D}

  \begin{algorithmic}[1]
  \STATE in progress
  \end{algorithmic}
  \label{algo:keep_algorithm_3D}

\end{algorithm}

\begin{algorithm}[h]
  \caption{Pseudocode the buffer extension algorithm}

  \begin{algorithmic}[1]
    \STATE $B = [\Lambda_k, 1, 1]$

    % store f_1
    \STATE $ \phi = \lfloor \frac{m}{(\Omega_k + \Lambda_k)\alpha} \rfloor $
    \IF{$\phi > \Theta_j$}
      \STATE $B_j \leftarrow \Theta_j$
    \ELSE
      \STATE $B_j \leftarrow \phi - 1$
      \STATE return $B$ //stop algorithm
    \ENDIF

    % store f_3
    \STATE $ \phi = \lfloor \frac{m - [\Omega_k \Theta_j](1-n)\alpha}{(n\Omega_k + \Lambda_k)\alpha} \rfloor $
    \IF{$\phi > \Lambda_j$}
      \STATE $B_j \leftarrow \Lambda_j$
    \ELSE
      \STATE $B_j \leftarrow \phi - 1$
      \STATE return $B$ //stop algorithm
    \ENDIF
    \IF{$B_j > \Theta_j$}
      \STATE $\textrm{storeF3} \leftarrow \textrm{True}$
    \ELSE
      \STATE $\textrm{storeF3} \leftarrow \textrm{False}$
    \ENDIF

    % store f_2
    \IF{$f_1 > f_2$}
      \STATE $g_1 = f_1$, $g_2 = f_2$, $g_3 = f_3$
      \STATE $\phi = \lfloor \frac{m}{\alpha[\Omega_k \Theta_j + n \Lambda_k \Omega_j + \Lambda_j \Lambda_k]} \rfloor$
      \STATE $A = F_1 + n(G_3 + F_2)$
    \ELSE
      \STATE $g_1 = f_3$, $g_2 = f_2$, $g_3 = f_1$
      \STATE $\phi = \lfloor \frac{m}{\alpha (\Theta_k \Omega_j + n\Omega_k\Lambda_j + \Lambda_j \Lambda_k)} \rfloor$
      \STATE $A = F_2 + n(G_3 + F_1)$
    \ENDIF

    \STATE $\Sigma = [g_1 + n(g_3 + g_2) + B_iB_jB_k]$
    \IF{$\Sigma < m$}
      \STATE continue
    \ELSIF{$\Sigma = m$}
      \STATE $\textrm{storeF2} \leftarrow \textrm{True}$
      \STATE return $B$ //stop algorithm
    \ELSE
      \STATE $\textrm{storeF2} \leftarrow \textrm{False}$
      \STATE return $B$ //stop algorithm
    \ENDIF

    % store F_1, F_2, F_3
    \IF{$\phi > \Theta_i$}
      \STATE $B_i \leftarrow \Theta_i$
    \ELSE
      \STATE $B_i \leftarrow \phi - 1$
      \STATE return $B$ //stop algorithm
    \ENDIF

    % store F_5 F_6 F_7
    \STATE $\phi = \lfloor \frac{\frac{m}{\alpha} - A + \Theta_iB}{(B + \Lambda_k\Lambda_j)} \rfloor$
    \IF{$\phi > \Delta_i$}
      \STATE $B_i \leftarrow \Delta_i$
    \ELSE
      \STATE $B_i \leftarrow \phi - 1$
      \STATE return $B$ //stop algorithm
    \ENDIF
    \IF{$B_i > \Theta_i$}
      \STATE $\textrm{storeF5F6F7} \leftarrow \textrm{True}$
    \ELSE
      \STATE $\textrm{storeF5F6F7} \leftarrow \textrm{False}$
    \ENDIF

    % store F_4
    $\Sigma = [G(1) + n(G(3) + G(2)) + N(G(4) + G(5) + G(6) + G(7)) + B_iB_jB_k]\alpha$
    \IF{$\Sigma > m$}
      \STATE $\textrm{storeF4} \leftarrow \textrm{True}$
    \ELSE
      \STATE $\textrm{storeF4} \leftarrow \textrm{False}$
    \ENDIF

  \end{algorithmic}
  \label{algo:buffer_extension}
\end{algorithm}

\subsection{Special case of non optimality}
When the amount of memory available is too small, a special case arise where the algorithm is not optimal.
Consider the 2D case where there is an overlap in the $k$ dimension only (see Figure \ref{fig:case_1_2}).
Let us say that the buffer size is such that $B_k = \Lambda_k$, but is too small to extend $B_j$ to $\Lambda_j$.
Keeping the extra data from the overlap in the $k$ dimension in memory allows us to write contiguously in the output files but it implies using more buffers in the $j^{th}$ axis.
In the worst case, $B_j = 1$ which incurs $\Lambda_j$ seeks.
As soon as $B_j > 1$ however (keeping $B_k = \Lambda_k$), we divide the number of seeks produced by writing in the incomplete output file per the number of buffers.
This observation is true for the other dimensions as well: as soon as $B_x = \Lambda_x$ in a given dimension $x$, the more we can read in the next dimension $x+1$, the better.
This proves that we should not increase a dimension if $B_x < \Lambda_x$ in the preceeding dimensions.

\subsection{Impact of the buffer order on performance}
Using the keep strategy in case of overlaps, one may order the buffer loadings to further reduce the number of seeks.
By optimizing the buffer ordering one can reduce the maximum quantity used to store the extra data in memory.
For example, if an overlap occurs only in the $k$ axis, loading the next buffers in this direction will enable recycling the extra data kept in memory, resulting in a smallest memory consumption over time.
The memory saved thanks to a smart ordering could enable the storage of more overlaps in memory using the ``keep strategy", further reducing the overall number of seeks. \\

\begin{figure*}[h]
\includegraphics[scale=0.2]{./figures/goodorderingbadordering.png}
\caption{ Comparison of a bad buffer ordering (left side) against a good buffer ordering (right side), given an overlap in the $k$ dimension.
The red area represents the amount of extra data kept in memory after loading the third buffer.
The right side order allowed to release the extra data from the first buffer after reading the second buffer.
}
\label{fig:goodorderingbadordering}
\end{figure*}

As we will see, the buffer ordering problem is complex and does not seem easily solvable.
Thanksfully, the impact of the buffer ordering on performance can be mitigated.
Indeed, the impact of the buffer ordering depends on the size of the falls, i.e. the overlaps between the buffer and the incomplete output files' shapes.
One can reduce the falls' sizes by using smallest chunks:
Even if the overlap between the input and output files is big with respect to their size, the area/volume of the falls will be kept small.
In particular, we remark that the falls tends to be smaller when the buffer shape is bigger than the output file shape, as the overlaps are smallest and concentrated on the borders (see Figure \ref{fig:goodorderingbadordering}).
We can stimulate this property by using small chunks such that we use buffer bigger input aggregates (see Stretching beyond the input aggregate shape), while keeping the overlaps small at the borders.

%----------------------------------------
\section*{A memory analysis of the keep strategy}
%----------------------------------------
The keep algorithm assumes that $m$ is big enough to keep at least a column of length $B_k$. % <- TODO: a revoir,
% above: cest faux car si input aggregate = input file on peut plus utiliser multiple reads/writes cas particuliers

As it has been explained, in 3D the buffer is then stretched in the $j^{th}$ dimension and finally in the $i^{th}$ dimension as long as there is enough main memory available to use the keep strategy.
This section covers how we estimate the amount of memory required by the keep strategy in order to know how much we can stretch the buffer in each dimension. \\

Two pieces of information are required to know the worst case memory consumption of the keep algorithm: what is the maximum number of each overlap area/volume we will have to keep in memory during the algorithm and what is the maximum size that those overlap areas/volumes can have.
In this analysis we express the overlap size in terms of the number of voxels that constitutes the overlap because the real quantity of memory used depends on the size of a voxel in memory.
For convenience one can avoid this extra parameter by setting the number of bytes per voxel to 1.

\subsection{Nomenclature}
Let us first define the nomenclature we will use (see Figure \ref{fig:nomenclature_overlaps}).
Given a buffer shape $B$ and the output file shape $O$, we define the overlap $C_k(x)$ as the overlap length in direction $k$ for the $x{th}$ buffer:
$$C_k(x) = xB_k mod(O_k)$$
$C_k(x)$ is not fixed during the buffer shape extension as it extends progressively.
We therefore define $\Omega_k(x)$ as the overlap length in direction $k$ if the buffer had shape $B=\Lambda$:
$$\Omega_k(x) = x\Lambda_k mod(O_k)$$
Finally, we define $\Theta_k(x)$ as follows:
$\Theta_k(x) = \Lambda_k - \Omega_k(x)$

\begin{figure*}[h]
\centering
\includegraphics[scale=0.45]{./figures/new/nomenclature_overlaps.png}
\caption{Figure (a) represents an input aggregate of shape $\Lambda$. The white cube represents the data that can be written directly into output files and the other blue blocks represent incomplete files. Figure (b) shows how the different volumes are indexed following the buffer order. Here the buffer order is the storage order. Another way of naming volumes is by using the decimal value or their positions, for example (0,0,1) is block 1, (1,1,0) is block 6.
}
\label{fig:nomenclature_overlaps}
\end{figure*}

\subsection{A ``naive" buffer ordering}
The maximum number of each overlap to keep in memory depends on the buffer order.
Therefore, the buffer order defines the maximum amount of extra data that we will have to keep in memory.
As we do not know a good ordering which do not imply a compute-intensive algorithm (see section ``Discussion"), let us define a naive ordering.
This naive ordering read the buffers in the direction where the overlap is the most important first, then read those buffer columns in the order of the second dimension where the overlap is the biggest, etc.
For example if the order is (j, k, i), we will read the buffers in direction j first.
This order reduces the number of big overlaps to keep in memory. \\

\begin{figure*}[h]
\centering
\includegraphics[scale=0.45]{./figures/new/naive_buffer_order.png}
\caption{On both figures the solid black borders blocks are buffers and the dotted red blocks are output files. On figure (a) an overlap in the first buffer order dimension is shown. One can see that each buffer recycles the previous extra data such that one never store more that the size of a buffer plus the size of $F_1$ in memory during the resplit process. Figure (b) shows that an overlap in the second dimension of the buffer order has to be kept in memory $b_k$ times with $b_k$ the number of buffers in the $k^{th}$ dimension. This can be extended to 3D by adding the overlaps in the third dimension of buffer order $N_s$ times with $N_s$ the number of buffers in a buffer slice.
}
\label{fig:naive_buffer_order}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[scale=0.45]{./figures/new/g_s.png}
\caption{Figure (a).  Figure (b).
}
\label{fig:g_s}
\end{figure*}

According to this ordering, we can now find the formula of the maximum number of overlap areas/volumes that will be stored during the keep algorithm execution:
If, for example, the ordering was $[k,j,i]$ (see Figure \ref{fig:naive_buffer_order}), we would keep at a maximum

$$\Sigma = (F_1 + n(F_2 + F_3) + N(F_4 + F_5 + F_6 + F_7) + B_iB_jB_k)\alpha$$

in memory.
$\alpha$ is the number of bytes per voxel i.e. the data type of the input and output files.
For the sake of simplicity we assume that the data types of all files are the same.
$n$ is the number of buffers in direction of the biggest overlap and $N$ is the number of buffers in a buffer slice in the direction of the second biggest overlap.

If we consider the overlap in the direction of the biggest overlap, direction $k$ on Figure \ref{fig:naive_buffer_order} (a), one can see that the extra data from the first buffer (in blue) is being recycled by the second buffer.
At any given time, the amount of memory used by the keep strategy is the size of the next buffer plus all the extra data that is pending for usage from the preceeding buffers.
If there is no overlap in the $j$ dimension one can see that the amount of memory to keep between the buffers is only $F_1$,
$\Sigma = (F_1 + B_iB_jB_k)\alpha$,
i.e. the overlap in $F_1$ is always recycled by the next buffer.
Note that after loading the last buffer in the $k$ dimension the last $F_1$ extra data stored is recyled.
Therefore one keep only one time $F_1$ at any given time in the resplit process.
With the same reasoning we can see that if the size of $F_2$ and $F_3$ is not 0 (Figure \ref{fig:naive_buffer_order} (b)) one will have to keep it until a complete buffer column has been processed such that the first buffer of the buffer column at buffer index $j+1$ will recycle the extra data from the first buffer from the column buffer at buffer index $j$.

This formula is for the 3D case, but it can be extended to the N dimensional case with the same reasoning.

\subsection{Extension of the buffer to match $\Lambda$}
\subsection{Extension of the buffer beyond $\Lambda$}
When extending the buffer more than $\Lambda$ we add $I_x$ to the dimension $x$ in which the amount of extra data is the most important while $B_x < R_x$.
If there is still some memory available after $B_x=R_x$, we start adding $I_y$ in the second dimension $y$ with the biggest amount of extra data, etc.
The above formulas still stands, it only changes the value of $B$. \\

By definition of the modulo, the maximum overlap length in a given dimension $x$ is $O_x$.
$O_x$ is an upper bound as in some cases like in Figure x we may never reach this point.
We could therefore compute $C_x(k)$ for all $k \in [1, b_x]$ to find the maximum if it is not too compute intensive in order to find the exact maximum length of the overlap in direction $x$. Now that we know how to compute the maximum $C$ value we can replace it in the above formula.

\subsection{Putting all together}
We can use this analysis to compute the buffer shape according to the following algorithm:

%----------------------------------------
\section*{Implementation}
%----------------------------------------

%----------------------------------------
\section*{Experiments}
%----------------------------------------

\begin{itemize}
  \item $dask_io$ vs naive with different shapes
  \item $dask_io$ perfs with different memory buffer sizes
  \item $dask_io$ perfs with different shapes
\end{itemize}

%----------------------------------------
\section*{Results}
%----------------------------------------

%----------------------------------------
\section*{Discussion}
%----------------------------------------

\subsection{Usage and support for MINC/Nifti file formats}
\subsection{Comparison with previous work}
\subsection{Solution of the ordering problem}
\subsection{Extending the algorithm for ROI extraction}
\subsection{Distributed keep algorithm}
\subsection{Improvements}

\begin{itemize}
  \item proof of optimality
  \item better ordering
\end{itemize}

%
% \subsection{Solution of the ordering problem}
%
% Our goal is to order the buffers so that we use the least memory as possible through the entire resplit process.
% In other words, we want to minimize the maximum amount of main memory used during the respit. \\
%
% \subsubsection{Memory deltas}
% One can model the sequential buffer loading problem as a doubly linked directed complete graph where each node is a buffer.
% Exchanges of data happen in main memory as we walk sequentially through the nodes using the ``keep strategy":
% At each node we get rid of some extra data loaded from previous buffers and we get some extra data from the new buffer.
% We call this difference between the amount of data in main memory before and after passing through a node a ``memory delta". \\
%
% \subsubsection{Modeling the problem as a shortest path problem}
% We will prove in the next paragraph that minimizing the maximum amount of memory during the resplit process is equivalent to finding the shortest path through all nodes of the previously defined graph with the ``distance" (weight on the edges between nodes) being the memory deltas.
% The algorithm must go through each node only once as we want to load all the buffers, only once (due to the non-overlaping buffer constraint).
% Note that weights can be negative if we give more extra data than we receive at a given node.
% Also note that weights update each time a buffer has been loaded:
% All the buffers that will recycle the extra memory from the current buffer see the weights of their incoming edges decrease. \\
%
% \subsubsection{Minimizing the maximum amount of memory is equivalent to a shortest path problem}
% The idea of the proof is that if the solution of one problem is the solution of the other, then they are be equivalent. \\
% To do. \\
% Note to self: An optimal solution of the shortest path problem with memory deltas as distances is a path that minimizes the sum of the memory deltas encountered during the process.
% % figure showing the sequence and the different cases
% Let us take such an optimal path and consider a sequence of the main memory used at each step on the path. \\
%
% \subsubsection{Conclusion}
% The buffer ordering problem is an all-pairs (we can start from any node and and at any node) dynamic (the weights update during the process) shortest-path problem.
% According to a review by Madkour et al.\cite{shorestpathsurvey}, Demetrescu and Italiano proposed a ``Fully dynamic algorithm over directed graphs for all-pairs shortest-paths with real-valued edge weights"\cite{demetrescu} to solve such a problem.
% Their solution seems to have been improved by Thorup in his paper ``Fully-Dynamic All-Pairs Shortest Paths: Faster and Allowing Negative Cycles"\cite{thorup}.
% The amortized update time is $O(n^2log^3(n))$ which is not a small overhead (Book of algorithm theory 2004).
% Proving the benefit of using such an algorithm reveals to be a complex time, that is why we decide to leave it as a future work and use a naive buffer ordering for the moment.
%
% \subsection{Comparison with previous work}
%
% \subsubsection{Split behavior of the algorithm}
% The split process is equivalent to the resplit process with the special case of having only one input file.
% In this case, the input aggregate has the same shape than the reconstructed image.
% The buffer will first be stretched in the $k$ direction until $B_k = \Lambda_k = \textrm{R}_k$.
% Then we will stretch in the $j$ dimension until $B_j = \Lambda_j$ and the same behavior applies in the $i^{th}$ dimension.
% Therefore, we first store complete rows, then block row tiles, then slices, block rows and slices.
% This algorithm is identical to the ``multiple reads" algorithm (see Hayot-Sasson et al.).
%
% \subsubsection{Merge behavior of the algorithm}
% The merge process is equivalent to the resplit process with the special case of having only one output file.
% In this case, the input aggregate has the same shape than the output file shape.
% The buffer will be stretched in the $k$ axis until $B_k = \Lambda_k = \textrm{R}_k$.
% Again, this is the same behavior than multiple reads.

%----------------------------------------
\section*{Conclusion}
%----------------------------------------

%----------------------------------------
\section*{Acknowledgments}
%----------------------------------------

\bibliography{Bibliography}
\bibliographystyle{ieeetr}

\appendices

  \section{Formulas for buffer extension}
  \label{FormulasKeep}

  \subsection{Overlap volume memory sizes}
  This appendix presents the reasoning and computations leading to the formulas used for the keep algorithm to find the biggest buffer shape possible given $m$, the amount of memory available. \\

  At step $x$, for $B(x)$ and the corresponding $\Omega(x)$ and $\Theta(x)$, we define the size of each volume (in number of voxels) as follows: \\
  $F_1 = (0,0,1) = \Omega_k min(B_j, \Theta_j) min(B_i, \Theta_i)$ \\
  $F_2 = (0,1,0) = \Theta_k max(0, min(B_j - \Theta_j, \Omega_j)) min(B_i, \Theta_i)$ \\
  $F_3 = (0,1,1) = \Omega_k max(0, min(B_j - \Theta_j, \Omega_j)) min(B_i, \Theta_i)$ \\
  $F_4 = (1,0,0) = \Theta_k \Theta_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\
  $F_5 = (1,0,1) = \Omega_k \Theta_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\
  $F_6 = (1,1,1) = \Theta_k \Omega_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\
  $F_7 = (1,1,0) = \Omega_k \Omega_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\

  For the sake of clarity we did not write the $(x)$ suffix ($B$ means $B(x)$).
  Also, we are only interested in the maximum amount of memory we would have to keep.
  To that aim, we will not evaluate the above volume formulas for each $x$ but only replace $\Omega_k(x)$ by its maximum and replace it in the formulas.
  By definition of $\Omega_k(x)$ an upper bound is $O_k$ but one can find the exact maximum value by computing $\Omega_k(x)$ for all $x$, for each dimension $k$.

  \subsection{Maximum amount of overlap to keep in memory}
  According to the naive buffer ordering, the maximum amount of volumes to keep during the execution is (see Section x):
  $$\Sigma = (G_1 + n(G_2 + G_3) + N(G_4 + G_5 + G_6 + G_7) + B_iB_jB_k)\alpha$$

  \subsection{Keeping f1}
  \noindent $g_1 = f_1 = \Omega_k B_j B_i = \Omega_k B_j$ \\
  $\Sigma = (g_1 + B_iB_jB_k)\alpha$ \\
  $\Sigma < m \Leftrightarrow (\Omega_k B_j + B_j \Lambda_k)\alpha < m$ \\
  $\Leftrightarrow B_j (\Omega_k + \Lambda_k) \alpha < m $ \\
  $\Leftrightarrow B_j < \phi$, $\phi = \lfloor \frac{m}{(\Omega_k + \Lambda_k)\alpha} \rfloor$

  \subsection{Keeping f3}
  \noindent $g_1 = f_1 = \Omega_k\Theta_jB_i = \Omega_k\Theta_j $ \\
  $g_3 = f_3 = \Omega_k (B_j - \Theta_j)B_i = \Omega_k(B_j - \Theta_j)$ \\
  $\Sigma = [g_1 + ng_3 + B_iB_jB_k]\alpha$ \\
  $\Sigma = [\Omega_k\Theta_j + n[\Omega_k(B_j -\Theta_j)] + B_j\Delta_k]\alpha$
  $\Sigma = [\Omega_k \Theta_j](1-n)\alpha + B_j(n\Omega_k + \Lambda_k)\alpha$
  $\Sigma < m \Leftrightarrow B_j < \lfloor \frac{m - [\Omega_k \Theta_j](1-n)\alpha}{(n\Omega_k + \Lambda_k)\alpha} \rfloor$

  \subsection{Keeping f2}
  \noindent $f_1 = \Omega_k \Theta_j B_i = \Omega_k \Theta_j $ \\
  $f_2 = \Omega_k \Omega_j B_i = \Omega_k \Omega_j $ \\
  $f_3 = \Theta_k \Omega_j B_i = \Theta_k \Omega_j $ \\

  We are asking ourselves if we can keep $F_2$.
  Remember that we only store all $F_2$ or nothing as it only saves 1 seek in 2D.
  If $F_1 > F_2$ we compute $\Sigma$ with $g_1 = f_1$, $g_2 = f_2$.
  In the other case, $g_3 = f_3$ and $g_1 = f_2$, $g_2 = f_3$, $g_3 = f_3$.

  $$\Sigma = [g_1 + n(g_3 + g_2) + B_iB_jB_k]$$

  \subsection{Increasing f1, f2, f3 to F1, F2, F3}
  \noindent $F_1 = \Omega_k \Theta_j B_i$ \\
  $F_2 = \Theta_k \Omega_j B_i$ \\
  $F_3 = \Omega_k \Omega_j B_i$ \\

  We are scaling the overlap areas from previous step into volumes.
  Therefore the buffer order is the same than at the previous step. \\

  If $k$ is the first buffer order ($g_1 =f_1$): \\
  $\Sigma = [G_1 + n(G_3 + G_2) + B_iB_jB_k]\alpha$ \\
  $\Sigma = [\Omega_k \Theta_j B_i + n(\Omega_k \Omega_j B_i + \Theta_k \Omega_j B_i) + B_iB_jB_k]$ \\
  $\Sigma = B_i\alpha[\Omega_k \Theta_j + n \Lambda_k \Omega_j + \Lambda_j \Lambda_k]$ \\
  $\Sigma < m \Leftrightarrow B_i < \lfloor \frac{m}{\alpha[\Omega_k \Theta_j + n \Lambda_k \Omega_j + \Lambda_j \Lambda_k]} \rfloor$ \\

  If $j$ is the first buffer order ($g_1 =f_2$): \\
  $\Sigma = [G_1 + n(G_3 + G_2) + B_iB_jB_k]\alpha$ \\
  $\Sigma = [\Theta_k \Omega_j B_i + n (\Omega_k \Omega_j B_i + \Omega_k \Theta_j B_i) + B_iB_jB_k] \alpha$ \\
  $\Sigma = B_i \alpha (\Theta_k \Omega_j + n\Omega_k\Lambda_j + \Lambda_j \Lambda_k)$ \\
  $\Sigma < m \Leftrightarrow B_i < \lfloor \frac{m}{\alpha (\Theta_k \Omega_j + n\Omega_k\Lambda_j + \Lambda_j \Lambda_k)} \rfloor$

  \subsection{Keeping F5, F6, F7}
  \noindent $F_1 = \Omega_k \Theta_j B_i$ \\
  $F_2 = \Theta_k \Omega_j B_i$ \\
  $F_3 = \Omega_k \Omega_j B_i$ \\
  $F_5 = \Omega_k \Theta_j (B_i-\Theta_i)$ \\
  $F_6 = \Theta_k \Omega_j (B_i-\Theta_i)$ \\
  $F_7 = \Omega_k \Omega_j (B_i-\Theta_i)$ \\

  $\Sigma = [G_1 + n(G_3 + G_2) + N(G_5 + G_6 + G_7) + B_iB_jB_k]\alpha$ \\

  We define A and B as follows: \\
  $A = G_1 + n(G_3 + G_2)$ \\
  $B = (\Omega_k \Theta_j + \Theta_k \Omega_j + \Omega_k \Omega_j)N$ \\
  $\Sigma = [A - \Theta_iB + B_iB + \Lambda_k \Lambda_j B_i]\alpha$ \\
  $\Sigma < m \Leftrightarrow B_i < \lfloor \frac{\frac{m}{\alpha} - A + \Theta_iB}{(B + \Lambda_k\Lambda_j)} \rfloor$

  \subsection{Keeping F4}
  We do not know which one of $F_4$, $F_1$, $F_2$ is the biggest overlap.
  They are 9 possibilities for the buffer order.
  If the order of biggest overlap is $i, j, k$ (Figure \ref{fig:g_s} (b)) as opposed to $k, j, i$ (Figure \ref{fig:g_s} (a)), the volume indices change.
  Our goal is to link the good $G$ to its corresponding $F$ volume size (see Figure \ref{fig:g_s}).
  To that aim we know that: \\
  1) Given $\beta{old}$ the base of storage order ($i,j,k$) we have the $F$ indices.
    $\beta{old} = (
    \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix},
    \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix},
    \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix})$ \\
  2) Any block index $(a,b,c)$ is in fact $(a*\beta[0] + b*\beta[1] + c*\beta[2])$ with $\beta$ the base used for indexing blocks. \\
  3) Saying that the buffer order is $i, j, k$ is equivalent to indexing the blocks using the base $(k,j,i)$. \\

  Therefore, let us say that we have a certain buffer order.
  Any index of a block $G$ in this base can be written $(a_{new}*\beta_{new}[0] + b_{new}*\beta_{new}[1] + c_{new}*\beta[2]_{new})$.
  To find the corresponding volume in the storage order (because we know the $F$ values) we can replace $\beta_{new}$ by $\beta_{old}$.
  The only information we need is $\beta_{new}$. This can be found by ordering $F_1, F_2, F_4$ by their volume size.
  If $F_1$ is the biggest volume, $\beta_{new}[0] = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ for example.
  At this point we just have to fill in a dictionary $G(i)$ associating index $i$ to the corresponding volume size.
  We then replace $G$ by $G(i)$ in $\Sigma = [G_1 + n(G_3 + G_2) + N(G_4 + G_5 + G_6 + G_7) + B_iB_jB_k]\alpha$.
  As for $F_2$ we either store all $F_4$ or we do not store it.
  If $\Sigma < m$ then we can store $F_4$ and move on to the final step of the algorithm. \\

\end{document}
