\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithmic, algorithm}
\usepackage{hyperref} % makes cross-refs (biblio, figures, algos, ...) clickable
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\logic}[1]{\color{red}\textbf{Logic/flow:}#1\color{black}}
\newcommand{\writing}[1]{\color{green}\textbf{Writing:}#1\color{black}}
\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:}#1\color{black}}
\newcommand{\timothee}[1]{\color{blue}\textbf{From Timoth√©e:}#1\color{black}}

\begin{document}

\title{ Dask Rechunk for sequentially splitting, merging and resplitting multidimensional arrays }

\author{\IEEEauthorblockN{Timoth\'ee Gu\'edon, Val\'erie Hayot-Sasson, Tristan Glatard \\
  \IEEEauthorblockA{
    Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada
  }}
}

\maketitle

\begin{abstract}
Todo.
\end{abstract}

\begin{IEEEkeywords}
multidimensional, array, split, merge, resplit, IO, processing, Dask, Python
\end{IEEEkeywords}

%----------------------------------------
\section{Introduction}
%----------------------------------------
% big data challenges
With the improvement of acquisition methods and the growth in the amount of data
available in several scientific domains such as health
sciences~\cite{bigdata_health}~\cite{Amunts1472}, geology~\cite{big_data_geology}
and astrophysics~\cite{biguniverse}, new big data challenges have emerged related
to the processing of large amounts of data, including ultra-high resolution
images. Big Brain, a human brain model providing microscopic data (20 micrometers) for
modeling and simulation~\cite{Amunts1472} is an example of ultra high resolution
images in the neuroscience field.

\subsection{Multidimensional array chunking}
% use of chunking
Scientific data is often represented as multidimensional arrays stored in
chunks to facilitate processing and storage. Among other advantages, chunking
allows for efficient queries, flexibility in adding new
data~\cite{optimal_chuking}, parallel processing, and out-of-core
computations through block algorithms.

% need for tools to rechunk
Chunking multidimensional array requires tools to efficiently split,
merge, and ``resplit" or ``rechunk" data files. Previous work in~\cite{seqalgorithms}
showed that naive algorithms to split an array into several chunks or merge
array chunks into one output file perform very poorly due to millions of seeks
occurring on disk.

% why subarray extraction incurs seeks
Indeed, retrieving sub-arrays from an array stored on disk incurs seeks in two situations: (1) when an array block
is opened for reading or writing, and (2) when the reading or writing process
moves within the block to access a contiguous piece of data. For a 3D cuboid of shape $C = (C_i, C_j, C_k)$
 stored in row-major order,
each column of data in the k dimension is contiguously stored. There are therefore
$C_j \times C_i$ data columns that all require a seek. Although the seek
time depends on various parameters such as the distance in bytes between two
data columns, we assume that all seeks incur the same time overhead for the sake
of simplicity. Therefore our problem will be to minimize the number of seeks
required to access data.

\subsection{The occurrence of seeks}
% explain storage order
\timothee{find references}
We assume that the data ordering in files is arbitrary but known.
For the sake of clarity and without loss of generality,
 we assume in this study that files are written in row-major order (a.k.a
``C" ordering), where the fastest moving dimension in the file is the last
dimension in the array, and the slowest moving dimension in the file is the first
dimension in the array. For example a 3D array with dimensions $i$, $j$ and $k$
would be written on disk by writing the complete columns in the k dimension first.
Our experiments use the HDF5 file format as it is commonly used in the
scientific community and it is written in \texttt{C-order} by default.

\tristan{Up to here, the intro is now well readable but it still needs some
editing. The paper should also be positioned w.r.t space filling curves and other
data ordering techniques on disk (see references from BigData 2017 paper).}

\subsection{Problem definition}
We focus on 3D arrays for simplicity. Consider a 3D array of shape $R =
(R_i, R_j, R_k)$, stored as input blocks of uniform shape $I =
(I_i, I_j, I_k)$. Our goal is to resplit the input blocks into output
blocks of uniform but different shape $O = (O_i, O_j, O_k)$.

A resplit algorithm (Algorithm \ref{algo:generalresplit}) takes as input a
list of input files \texttt{inFiles} of shape $I$, a list of output files
\texttt{outFiles} of shape $O$ and the amount of memory \texttt{m}
available for the resplit. Based on a given strategy and taking $m$ into
account, the algorithm first defines an ordered list of buffers positions
\texttt{buffersList} defining a partition of array $R$. The buffers are
loaded sequentially, and their data is stored in a cache. After each buffer
load, the complete output files stored in cache are retrieved and written
to output files. Output files may be covered by one or more buffers. \tristan{To increase clarity,
you should number lines in Algo 1 and refer to line numbers in the above paragraph.}

For simplicity, we require that all buffers in \texttt{buffersList} have
the same shape $B$. Our problem is to find the partition \texttt{bufferList}
that minimizes the number of seeks done by Algorithm~\ref{algo:generalresplit}
given $I$ and $O$, subject to the amount $m$ of available memory.

\tristan{you should name the problem here (``rechunk problem''), and perhaps write it as an optimization problem.}
\tristan{explain that seeks are done by the read function, + 1 in the write. Explain that the size of cache has to be bounded by m.}
\tristan{mention that a solution of this problem implements getBufferPositions}

A lower bound on the number of seeks for the resplit task is $n_i + n_o$,
with $n_i$ the number of input files and $n_o$ the number of output files.
Indeed, input and output files all have to be accessed at least once, which
requires a seek.

\begin{algorithm}
  \caption{General resplit \tristan{Use resplit or rechunk consistently} algorithm}
  \label{algo:generalresplit}
  \begin{algorithmic}
    \STATE \textbf{Inputs:} inFiles, outFiles, $m$
    \STATE buffersList $\leftarrow$ getBufferPositions($I$,$O$,$m$)
    \STATE initialize(cache)
    \FOR{buffer in buffersList}
      \STATE bufferData $\leftarrow$ read(inFiles, buffer)
      \STATE cache.insert(bufferData)
      \FOR{($i$, data) in getCompleteFiles(cache)}
        \STATE write(outFiles[$i$], data)
      \ENDFOR
    \ENDFOR

  \end{algorithmic}
\end{algorithm}

\subsection{The multiple and clustered strategies}
Special cases of the rechunk problem occur when $I=R$ (``split'' problem)
or $O=R$ (``merge'' problem). Two strategies have been introduced
in~\cite{seqalgorithms} to address these problems. Focusing on the split
problem $(I=R)$, the naive algorithm defines the buffer positions to be
\tristan{write them formally}, such that subarrays of shape $O$ are
iteratively loaded from $R$ and written to an output
file~\cite{seqalgorithms}.

The clustered strategy loads contiguous blocks of data of shape $O$ \tristan{it would be useful to express this using the notations of Algo 1 (what is in the cache, what does getBuffersPositions do, etc).}
from the input array in order to write it in one seek in each output file which
data has been loaded. This strategy loads data blocks one by one, block columns
by block columns or blocks slices by block slices, depending on the amount of
main memory available for the buffer. Using this strategy for resplitting would
imply reading multiple input files that are contiguous in the reconstructed
image or reading the data of contiguous output files by seeking into input files.
In either case one can find special cases in which the algorithm seeks in all
dimensions.

The multiple strategy aims at not doing any seek while reading and writing. For
example, one would read the input file slice by slice and write these slices
contiguously in the output files \tristan{it would be useful to express this using the notations of Algo 1.}. The tradeoff lies in switching between files
at each buffer loading. Using this strategy for the resplit task would imply
reading contiguous columns or slices of data from the input files and writing it
contiguously in the output files. This would result in even more switches
between input and output files and one can easily find worst cases when using
small $I$ and $O$ shapes for example where a smarter strategy would be appreciated.

To the best of our knowedge however, no algorithm has been proposed for the
resplit task. The algorithm presented in this study can be seen as an
adaptation of the multiple strategy to the resplit task.

\tristan{The previous paragraph should use the notations in Algo 1.}

\subsection{Baseline solution}

A baseline/naive algorithm for the resplit task would be to load one input file
at a time. It is equivalent to computing the buffers' positions in
Algorithm~\ref{algo:generalresplit} using a buffer shape equal to $I$.
We assume that one input file can stand in memory ($m \geq I_iI_jI_k$). Each
buffer is loaded only once and the data loaded into the cache is directly
written down into the appropriate output files. The buffer order has no
implication on the number of seeks, so let us set the buffer order arbitrarily
to the storage order ($i \rightarrow j \rightarrow k$ in row-major for example).

We can compute the amount of seeks $s$ produced by such an algorithm with
Equation~\ref{eq:1}. The equation is composed of three terms, each term represents
the amount of seeks in on of three distinct cases (1) $I_k \neq O_k$, (2)
$I_k = O_k$ and $I_j \neq O_j$ and (3) $I_k = O_k$, $I_j = O_j$ but $I_i \neq O_i$.
These cases are described using $\alpha$, defined as: \\
$\alpha_x = \begin{cases}
   1 & if I_x \neq O_x \\
   0 & else
\end{cases}$
\begin{itemize}
  \item $d_x$ is the number of shape mismatches between the output and output files
  in direction $x$. For example in direction $k$ we count one shape mismatch for
  each upper border of an input file in direction $k$ that does not match with
  an output file border in direction $k$, and vice-versa.
  \item $R_x$ is the length of the reconstructed image in dimension $x$
  \item $n_x$ is the number of input files in dimension $x$.
\end{itemize}

\begin{multline} \label{eq:1}
s = [(d_k+1)R_iR_j]^{\alpha_k} \\ + [(d_j+1)R_in_k]^{(1-\alpha_k)\alpha_j} + [(d_i+1)n_jn_k]^{(1-\alpha_k)(1-\alpha_j)}
\end{multline}

As one can see from Equation~\ref{eq:1}, unless the dimensions of the input and
output files match, a considerable amount of seeks will occur. Also, a shape mismatch
in the $k$ dimension is more costly than a shape mismatch in the $j$ dimension,
which is itself more costly than a mismatch in dimension $i$ (assuming that the
files are stored in C-order).

\subsection{Dask}

Dask is a popular Python package enabling parallel and out-of-core
computation in the SciPy ecosystem. It represents computations as task
graphs that are dynamically executed by one of several schedulers offered
by Dask including the single-threaded, the multi-threaded, the
multi-process, and the distributed schedulers. Custom schedulers can also
be implemented. Dask graphs can be used out-of-the-box or through built-in
APIs. For example, \texttt{dask.array} is a parallel and out-of-core Numpy
clone, and
\texttt{dask.dataframe} is a Pandas clone. A Dask graph is implemented in
plain Python as a dictionary with any hashable as keys and any object as
values. More precisely, a "Value" is any object different than a task and a
"Task" is a tuple with a callable as first element.

We focus on the \texttt{dask.array} collection, a data structure designed for
multi-dimensional array processing using blocked algorithms.

\subsection{Contributions}
This paper makes the following contributions:
\begin{itemize}
  \item Definition of the resplit problem for multidimensional arrays
  \item A sequential algorithm to efficiently resplit multidimensional arrays
  \item An open implementation of this algorithm in Dask, in a library called Dask Rechunk.
  \item An open implementation in Dask of the ``clustered" split and merge strategy described in~\cite{seqalgorithms}, in Dask Rechunk too.
\end{itemize}

%----------------------------------------
\section{The ``keep" algorithm}
%----------------------------------------

\subsection{Considerations on shape mismatch}
We call shape mismatch a rechunk problem where $I_x \neq O_x$ in a given dimension $x$. As it
has been shown in the paragraph on the baseline algorithm \tristan{ambiguous and heavy: use either ``as mentioned previously'' or ``as mentioned in section x.y.z''}, if $I_x < O_x$ and
we read one file at a time \tristan{unclear: express using the notations of Algo 1}, we are bound to seek in the output files at
write time. If, however, $I_x > O_x$, then we get remainders \tristan{remainders weren't defined}. Finally, if we
just read what we need from the input files to write in output files
contiguously, we seek while reading and writing. A solution to reduce the
number of seeks seems to be to read more than $O_x$ and to elaborate a strategy
to keep the remainders in memory instead of writing it down directly \tristan{this is what the cache does in Algo 1} until we
get enough data to write it with a reduced amount of seeks. Such a strategy is
presented in this section.

\tristan{The previous paragraph is unclear because Algo 1 already uses a cache. I think the point we want to make is that
naive algorithms would empty the cache at each iteration, while a ``keep'' strategy doesnt.}

\subsection{The keep strategy}
The keep strategy uses a cache to keep remainders in memory.
The strategy requires an algorithm to find the best buffer shape in order to
always try to have remainders, i.e. $B_x>O_x$. As we are constrained by the
amount of memory available, it may be that we cannot keep some of the remainders.
In this case those remainders are directly written down into the appropriate
output files to avoid re-reading input files several times and ensure that all
the data has been written at the end of the algorithm. Being able to keep all
remainders into memory to write an output file only when all the data has been
loaded in memory means doing the minimum number of seeks possible during a
resplit task ($n_i + n_o$). In the following paragraphs we explain how to find
the best buffer shape for to the keep strategy.

\tristan{This is also a bit redundant with the formulation in Algorithm 1.
I think there should rather be an addition to Algo 1 to flush the cache to disk when it is too big.}

\subsection{Input aggregates}
As explained, our goal is to find a buffer shape such that $B_x>O_x$ in any
direction $x$ to be able to use the keep strategy hence reducing the number of
seeks. If $I_x > O_x$, then $B_x$ can be set to $I_x$. If $I_x < O_x$ however,
one needs a buffer that is a multiple of $I_x$ in order to minimize the number
of seeks at read time and have remainders ($B_x>O_x$). We define an input
aggregate as being the minimum aggregate of input files that covers one output
file completely. In particular, it is the input aggregate that covers the first
output file (indexed $(0,0,0)$) in the storage reconstructed image. The best
buffer shape for the keep strategy is therefore the shape of an input aggregate.
We call this shape $\Lambda$.

\subsection{Stretching the buffer in the storage order}
To get a buffer close to $\Lambda$ we define an algorithm that stretches the
buffer shape step by step. At each step, the algorithm estimates how much the
buffer can be stretched in one direction while keeping the remainders into
memory. Thhe problem of what dimension to increase first still stands. A
strategic order seems to be to stretch the buffer in the direction which saves
the maximum number of seeks first. Given the analysis of the baseline algorithm,
such an order is the storage order as in the case of a row-major ordering such
an order would keep remainders in the $k$ dimension first. An analysis of the
memory consumption of the keep strategy at each step together with the associated
algorithm is given in the next section.

\subsection{Impact of the buffer order on performance}
Using the keep strategy in case of overlaps, one may order the buffer loadings
to further reduce the number of seeks. By optimizing the buffer ordering one can
reduce the maximum quantity used to store the extra data in memory. For example,
if an overlap occurs only in the $k$ axis, loading the next buffers in this
direction will enable recycling the extra data kept in memory, resulting in a
smallest memory consumption over time. The memory saved thanks to a smart
ordering could enable the storage of more overlaps in memory using the
``keep strategy", further reducing the overall number of seeks.

As we will see, the buffer ordering problem is complex and does not seem easily
solvable. Thanksfully, the impact of the buffer ordering on performance can be
mitigated. Indeed, the impact of the buffer ordering depends on the size of the
remainders. One can reduce the remainders' sizes by using smallest chunks: Even
if the overlap between the input and output files is big with respect to their
size, the area/volume of the remainders will be kept small. In particular, we
remark that the falls tends to be smaller when the buffer shape is bigger than
the output file shape, as the overlaps are smallest and concentrated on the
borders (see Figure \ref{fig:goodorderingbadordering}). We can stimulate this
property by using small chunks such that we use buffer bigger input aggregates
(see Stretching beyond the input aggregate shape), while keeping the overlaps
small at the borders.

\begin{algorithm}
  \caption{Pseudocode of the keep algorithm}
  \label{algo:keepalgorithm}
  \begin{algorithmic}
    \STATE \textbf{Inputs:} inFiles, outFiles, $m$
    \STATE volumesToKeep, buffersList $\leftarrow$ bufferExtension($I$, $O$, $m$)
    \STATE cache.initialize()
    \FOR{buffer in buffersList}
      \STATE bufferData $\leftarrow$ read(inFiles, buffer)
      \STATE write(bufferData, outFiles, volumesToKeep, cache)
      \STATE cache.insert(bufferData, outFiles, volumesToKeep)
      \FOR{$i$, data in cache}
        \IF{outFiles[$i$] is complete}
          \STATE write(outFiles[$i$], data)
          \STATE cache.remove($i$)
        \ENDIF
      \ENDFOR
    \ENDFOR

  \end{algorithmic}
\end{algorithm}

\tristan{Algo 3 is now quite redundant with Algo 1, here you should write only the getBufferPositions function.}

%----------------------------------------
\section{A memory analysis of the keep strategy}
%----------------------------------------
This section covers how we estimate the amount of memory required by the keep
strategy to know how much the buffer can be stretched in a given direction while
keeping the remainders in memory. In this analysis, we express the quantity of
memory used by an array as the number of voxels it contains. It is equivalent
to saying that the number of bytes per voxel is 1.

The amount of memory required is the maximum memory consumption reached during
the execution of the keep algorithm. To estimate this amount, we first need to
define what a remainder volume is. A buffer of shape $\Lambda$ can be divided in
8 parts or ``volumes". 7 out of those 8 volumes are remainder volumes because
they represent the overlap between the input aggregate and the output files on
its border. These 7 remainder volumes enclose an 8th part that is composed of
input files that are either complete or which complementary part have already
been loaded by a previous buffer. This means that any complementary part is either
in memory (it has been kept in cache according to the keep strategy) or it has
been written down previously. Therefore, this 8th volume cannot be kept by the
keep strategy and is not considered a remainder. For each buffer, each of volume
is indexed following the buffer order (see section on buffer order) $G_0$ to $G_7$, with
$G_0$ being the non-remainder volume. If the buffer is smallest than the input
aggrgegate or if there is no shape mismatch in a given dimension, it may be that
a volume size is set to 0.

Having partitioned the buffers into such volumes, we can see that the maximum
memory consumption is computed from two pieces of information: The maximum number
of each volumes we must keep during the process and the maximum size of each
volume. The maximum number of each volumes that we must keep during the process
is (see computation details in Appendix~\ref{bufferExtensionAlgorithm}):

\begin{equation} \label{eq:2}
\sigma = F_1 + n(F_2 + F_3) + N(F_4 + F_5 + F_6 + F_7)
\end{equation}

With $n$ the number of buffers in the first direction of the buffer order and
$N$ is the number of buffers in a buffer slice, i.e. in the second direction of
the buffer order. Given that at each step of the resplit algorithm one must
load a buffer, we should also add the size of a buffer to the equation:

\begin{equation} \label{eq:3}
\Sigma = (F_1 + n(F_2 + F_3) + N(F_4 + F_5 + F_6 + F_7) + B_iB_jB_k)
\end{equation}

Finally, to find the maximum size of each volume one must know the maximum
overlap length between the buffer and the output files on its border. One can
either compute all possibilities and take the maximum in each dimension if it is
not too costly or use the following upper bound: By definition of an input
aggregate, the overlap between any buffer and its bordering output files it at
maximum Ox in dimension x.

Given equation~\ref{eq:2} and the maximum size of the volumes, we can now
stretch the buffer shape one dimension at a time using simple equations to ensure
that the maximum amount of memory consumed during the process wil stay below the
amount of memory available. Such computations and the associated buffer stretching
algorithm are given in Appendix~\ref{bufferExtensionAlgorithm}.

%----------------------------------------
\section{Methods}
%----------------------------------------
\tristan{This can't be called methods, the algo definition is already methods.}
Before diving into the implementation of the keep algorithm we first designed
an experiment to prove that dask.array was subject to the seek problem. To that
aim we created a random array from a uniform distribution and stored it as a
HDF5 file for later processing. The array had a shape of x which represents 1/8
of the Big Brain size. The experiment then consisted in splitting and merging
the array with dask using two different chunk shapes: one of them doing more
seeks than the other. We did this experiment on both HDD and SSD to see if the
results were also visible when using an SSD which is suposedly less affected by
seeks. The results of this experiment showed that dask was indeed subject to the
seek problem. See the "Results" section for more details.

In order to see if a strategy to reduce the number of seeks was effective, we
implemented and tested the "clustered" strategy for splitting and merging
multidimensional arrays. We chose this strategy first because it seemed simpler
to implement and study~\cite{seqalgorithms} showed that such a strategy was
working. It means that if it failed the problem would be more likely to come
from the implementation or dask than from the algorithm itself. We also know
from~\cite{seqalgorithms} that the difference between the naive and clustered
strategies should be large enough to be visible in the processing time. The
shapes used for the experiment are shown on the table x. The implementation
details of the clustered strategy are discussed in the appropriate section and
again, the results are presented in the "Results" section. The code of the
experiment is available in a dedicated repository on Github.

After these two preliminary experiment, we implemented a simplified version of
the keep algorithm due to the difficulty to implemlent such an algorithm in a
task graph (see section "Implementation details"). In order to evaluate the
efficiency of the keep algorithm we compare it against vanilla dask on local
computer. To be fair we used dask with one thread as required for our
implementation but also with the classic multi-threaded  scheduler as it is the
default when using dask.array. In order to compare both implementations to a
naive resplit, we also added a plain Python implementation to the benchmark. For
the experiments presented below we built a random 3D array of the size of Big
Brain, drawn from a uniform distribution. Once again, the array is initially
stored into an HDF5 file.

We did three experiments on the keep algorithm. The first one consisted in
comparing the different implementations in the case where the keep algorithm is
supposed to be the most effective i.e. when the buffer is equal to the input
aggregate $B_x=\Lambda_x$. In particular, three interesting cases are when
$B=(1,1,\Lambda_k)$, $B=(1,\Lambda_j, \Lambda_k)$ and $B=\Lambda$. Those three
cases represent ideal cases with different amount of data available. This
experiment aimed at (1) showing if the keep algorithm worked, (2) seeing how much it
can improve the processing speed and (3) how much gain it represents to be able to
stretch the buffer in each dimension.

In the second experiment, a more realistic case has been tested. In this case,
$O~=I$ which is more likely to happen in real life. Although in a realistic
scenario $O$ and $I$ would be relatively small, we also tested bigger shapes to
see the impact on performance and if one implementation was more resilient than
another.

Finally, the particular case of an important shape mismatch was tested. The cases
where $O>>I$ and $O<<I$ are equivalent, as in both cases the input aggegate
will be quite big. This is supposed to be a bad case for the keep algorithm
as extending the buffer to $\Lambda$ will be more complicated. For this
experiment, we fixed $m$ and $I$ and then ran multiple times the algorithms
while increasing $O$.

%----------------------------------------
\section{Implementation details}
%----------------------------------------

\subsection{Clustered strategy}

As explained in the introduction, daks represents computations by a task graph.
Implementing the clustered strategy consisted in modifying that graph. To that
aim, we implemented an optimization function to create "buffer" tasks in the
graph.

The optimization starts by converting the task graph into a mathematical
directed graph for use by a BFS (Breadth First Search) algorithm. More
precisely, we implement it as adjacency lists. The BFS algorithm allows us to
find and store tasks by type for later processing. Assuming that the whole array
is being processed, we find the appropriate buffering scheme from $I$, $O$ and
$m$, according to the algorithm in~\cite{seqalgorithms}. The buffering scheme
specifies if the algorithm loads buffers block by block, block columns by
block columns or block slices by block slices.

We then update the data loading tasks to force dask to load a whole buffer at
a time instead of small chunks. Finally, we modify the dependent tasks to make
them use our new buffer tasks. According to the clustered algorithm, all buffers
do not necessarily have the same shape, that is why we could not use the
\texttt{dask.array.rechunk} method.

The default scheduler for dask.array is the multi-threaded scheduler. It means
that at the beginning of the execution dask dispatches some tasks that have
no dependencies into different threads. The problem is that the clustered
strategy tries to use all the memory available for each buffer, and such buffer
tasks are the one with no initial dependencies. Therefore, we needed to
modify dask's scheduler to make it load one buffer only after the previous one
and its dependent tasks have ended. This avoid running out of memory during the
processing.

\subsection{The keep algorithm}

The buffer extension algorithm described in Appendix A has been direclty
implemented in plain Python. Implementing the last part of the algorithm
consisted in: (1) creating the buffer tasks, (2) dispatching the different data
parts loaded by each buffer into the appropriate output files, (3) simulating
the cache to keep remainders in memory between buffer loadings.

This time, the buffer task creation was straightforward as it is assumed by the
algorithm that the buffers constitute a partition of the whole array. For the
second and third part, we used the \texttt{dask.array.store} method that
requires input arrays as argument, together with the output files in which to
store the data and the indices of each data part in the output files. To
automatically compute such information, we proceeded as follows. We first
compute, for each output file, the list of each constitutive subarrays, with
subarrays defined by the intersection of input and output files and buffers.
When each output file ``knows" which part from which buffer are required, we
merge some of them according to the keep strategy to enforce the fact that
a given data part of a given output file has to be stored at once. By doing this
we create dependencies in the task graph. Such dependency tells dask that it
needs to load two buffers before writing a given subarray for example.

\subsection{Dask Rechunk}

Both implementations are available in a same Python package called \texttt{Dask Rechunk}. It
contains tests with 99\% coverage and is available on PyPI (The Python Package
Index). Dask Rechunk provides a simple API to use the split, merge and resplit
algorithms described in this study (figure x).

%----------------------------------------
\section{Results}
%----------------------------------------

%----------------------------------------
\section{Discussion}
%----------------------------------------

\subsection{The buffer ordering problem}
The buffer ordering problem can be modeled as follows: We can reprensent the
problem as a complete bidirectional graph in which each vertex is a buffer. At
initialization, we must choose a first buffer as an entry point. Let us define
a path in the graph as visitation order of all buffers in the graph. One buffer
cannot appear twice in the list. Finally, let us define a cache that contains
the data currently in main memory. We define the buffer ordering problem as the
problem of finding the optimal path such that the amount of memory used by the
cache is kept minimal during the process. For each buffer, we load some data
into the cache and free some of it (writing into output files). The amount of
data released is different depending of the buffers previously visited and each
buffer can only be visited once which can be reprensented by removing all edges
pointing to a visited vertex except the one coming from the previously visited
vertex. One may solve this problem by using a greedy algorithm but it would
incur more infering at runtime and a potentially complex algorithm to run.
We decided to keep things simple using a naive buffer order, the storage order,
as the goal of this study is primarily to assess if the keep algorithm works.

\subsection{Breaking the buffers}
To even reduce the maximum amount of memory used during the resplit process, we
could read the input files one by one instead of enforcing dask to read a whole
buffer at a time, provided that the input files' parts are read in the right
order (following the buffer order). The idea of reading an entire buffer at a
time came from the clustered and multiple strategies but we realized afterwards
that this could be beneficial to break the buffers into input files' parts. We
can see in Equation (2) that it would result in replacing $B_iB_jB_k$ by
$I_iI_jI_k$ or less if part of a file is loaded. This idea would only be
beneficial in the case where $B_x>I_x$. Indeed, in the other case, the buffer is
loaded anyways.

\subsection{ROI extraction problem}
The Region Of Interest (ROI) extraction problem is a related problem that still needs
to be adressed. A solution using chunking as been introduced in []. The authors
define an array partitioned into chunks of equal shapes and then define a
query as an arbitrary subarray of the input, chunked, array. They define the
optimal chunking problem as finding the optimal chunk such that the expected
number of chunks retrieved to answer the query is minimal. In our opinion, the
solution in [] is limited due to the need of historical or theoretical workload
and the necessity to rechunk the input array into an ``optimal" chunk shape. We
would prefer letting the application choose the appropriate chunk shape
regarding its needs and not needing to estimate the processing workload. We
define the ROI extraction problem as follows: Finding an algorithm that takes
as input an arbitrary chunk shape and extract the ROI data from the chunks with
the less number of seeks as possible.

\subsection{Solving three problems at once}
As stated in the introduction, the split and merge tasks are special cases of
the resplit task. This observation leads us to think that maybe one could find
one optimal algorithm for the split, merge and resplit tasks.

If we were to use the keep algorithm with $I=R$, we would read the input data
in slices, exactly like the multiple strategy. The only difference between the
two strategies, however, is that if some remainders appear at the bottom of the
buffer, the keep algorithm would keep it to try to read and write files in one
seek. This is what we meant by "a smarter adaptation of the multiple strategy
to the resplit task": We not only try to limit seeking into the files but we
also try to limit switching between the files. It would be interesting to
compare the two algorithms for the split/merge tasks to see if the keep
implementation brings any kind of improvement.

\subsection{Towards distributed systems}
A future work would be to find distributed versions of the split/merge/resplit
algorithms. Lots of scientists use HPC (High Performance Computing) clusters
regularly which brings considerations about how to use such distributed
algorithms with Lustre for example, a commonly used filesystem for HPC. Dask
also provides a distributed scheduler that seems to be quite efficient. It is
now recommended for use, even on local computers (using one node).

%----------------------------------------
\section{Conclusion}
%----------------------------------------

%----------------------------------------
\section{Acknowledgments}
%----------------------------------------

\bibliography{Bibliography}
\bibliographystyle{ieeetr}

\appendices

\section{Buffer extension algorithm}
\label{bufferExtensionAlgorithm}
The buffer extension algorithm aims at extending the buffer shape from
(1,1,$\Lambda_k$) to the input aggregate shape $\Lambda$. Note that we assume
that $m$ is large enough to store a buffer of shape (1,1,$\Lambda_k$). It means
that we can read one complete data column from an input file and keep the
remainder in memory for the next buffer. If it was not the case the keep
algorithm would not give a significant improvement in the number of seeks
produced by the resplit task, compared to the baseline algorithm. In all the
computations below, we measure the amount of data as a number of voxels. With
$\alpha$ the number of bytes per voxel in the storage device, $m$ is defined as
$m/\alpha$ in the following computations.

In the buffer extension algorithm, the buffer is stretched in one dimension at a
time. As explained in section ``A memory analysis of the keep strategy", each
buffer can be divided into 8 volumes, 7 of which are \texttt{remainder} volumes.
When stretching the buffer, we compute the maximum length of the buffer possible
in the dimension considered, while ensuring that the remainder volumes can be
kept into memory according to the \texttt{keep} algorithm. To that aim, we need
a formula that computes the maximum amount of times that we have to keep each
remainder volume, together with the maximum size of those volumes. For example, if
we consider stretching the buffer in the $j$ dimension, we want to know how big
$B_j$ can be such that the remainders are kept in memory by the \texttt{keep}
algorithm for later use, given the maximum amount of memory available, $m$.

We define $\Sigma$ as the maximum amount of data reached during the execution
of the keep algorithm in terms of the number of volumes to be kept. From the
previous sections we assume that the buffer order is the storage order. In this
order, the $F_1$ volume stored from a given buffer loading is necessary recycled
in the next one. Therefore there can be only one $F_1$ volume in cache at a
given time. For any buffer, $F_2$ and $F_3$ are kept in memory until the next
buffer in the $j$ dimension recyles them. Therefore they can be present in cache
$n$ times at a maximum, with $n$ the number of buffers in a buffer column (the
number of buffers in dimension $k$). Volumes 4 to 7 can only be recycled by the
next buffer in the $i$ dimension. Therefore, the algorithm must read an entire
buffer slice before being able to use them from the cache. They are kept a
maximum of $N$ times, with $N$ the number of buffers in a slice of the
reconstructed image. Last but not least, a buffer must be loaded at each step of
the keep algorithm. This gives us the following formula for $\Sigma$:

$$\Sigma = (F_1 + n(F_2 + F_3) + N(F_4 + F_5 + F_6 + F_7) + B_iB_jB_k)\alpha$$

In order to compute the volumes' sizes, we need to define some nomenclature.
Given a buffer shape $B$ and the output file shape $O$, we define $C_d(x)$ as
the overlap length between the buffer and an overlaping output file in direction
$d$ for the $x{th}$ buffer. Let us define $\Omega$ and $\Theta$, that are used
to define upper bounds in the volumes sizes formulas. $\Omega_d(x)$ is the
equivalent of $C_d(x)$ if the buffer was of shape $\Lambda$, and $\Theta_k(x)$
is the difference between $B_d$ and $\Omega_b$, with $B=\Lambda$.
These metrics are computed as follows:

$$C_d(x) = (x+1)B_d mod(O_d)$$
$$\Omega_d(x) = (x+1)\Lambda_d mod(O_d)$$
$$\Theta_d(x) = \Lambda_d - \Omega_d(x)$$

For the $x^{th}$ buffer, the volumes' sizes are:
$F_1 = \Omega_k min(B_j, \Theta_j) min(B_i, \Theta_i)$ \\
$F_2 = \Theta_k max(0, min(B_j - \Theta_j, \Omega_j)) min(B_i, \Theta_i)$ \\
$F_3 = \Omega_k max(0, min(B_j - \Theta_j, \Omega_j)) min(B_i, \Theta_i)$ \\
$F_4 = \Theta_k \Theta_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\
$F_5 = \Omega_k \Theta_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\
$F_6 = \Theta_k \Omega_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\
$F_7 = \Omega_k \Omega_j max(0, min(B_i-\Theta_i, \Omega_i))$ \\

We are only interested in the maximum amount of memory that will be reached by
the algorithm. To that aim, we will not evaluate the above formula for each $x$
but we will replace each $\Omega_d(x)$ by its maximum in $\Sigma$. By definition
of $\Omega_d(x)$, an upper bound is $O_d$ but one can find the exact maximum
value by computing $\Omega_d(x)$ for all $x$, for each dimension $d$.

In the following paragraphs, we are finding formulas to extend the buffer one
dimension at a time, for use in the buffer stretching algorithm.

\subsection{Keeping F1}
We first extend the buffer in the $j$ dimension.

\noindent $\Sigma = (F_1 + B_iB_jB_k)\alpha$ \\
$F_1 = \Omega_k B_j B_i = \Omega_k B_j$ \\
$\Sigma < m \Leftrightarrow (\Omega_k B_j + B_j \Lambda_k)\alpha < m$ \\
$\Leftrightarrow B_j (\Omega_k + \Lambda_k) \alpha < m $ \\
$\Leftrightarrow B_j < \phi$, $\phi = \lfloor \frac{m}{(\Omega_k + \Lambda_k)\alpha} \rfloor$

\end{document}
